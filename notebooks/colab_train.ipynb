{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü´Å Pneumothorax AI ‚Äî Global Pre-training\n",
    "**T√úBƒ∞TAK 2209-A | Ahmet Demir | Dokuz Eyl√ºl √úniversitesi**\n",
    "\n",
    "**Veri stratejisi ‚Äî Google Drive:**\n",
    "- NIH verileri Drive'a bir kez indirilir, sonraki oturumlarda tekrar indirilmez\n",
    "- T√ºm checkpoint'ler Drive'a kaydedilir\n",
    "- Drive path: `MyDrive/tubitak_pneumothorax/`\n",
    "\n",
    "**Tahmini s√ºre:** T4 GPU ile ~12 saat (50 epoch, 112k g√∂r√ºnt√º)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. GPU Kontrol√º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'YOK ‚Äî Runtime > GPU se√ß!')\n",
    "print('CUDA:', torch.version.cuda)\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Google Drive Baƒüla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom google.colab import drive\n\ndrive.mount('/content/drive', force_remount=True)\n\n# Drive klas√∂r yapƒ±sƒ±\nDRIVE_BASE = '/content/drive/MyDrive/tubitak_pneumothorax'\nDRIVE_SIIM = f'{DRIVE_BASE}/data/siim'\nDRIVE_CKPT = f'{DRIVE_BASE}/checkpoints'\n\nfor d in [DRIVE_SIIM, DRIVE_CKPT]:\n    os.makedirs(d, exist_ok=True)\n\nprint(f'‚úì Drive baƒülandƒ±')\nprint(f'  SIIM data : {DRIVE_SIIM}')\nprint(f'  Checkpoint: {DRIVE_CKPT}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Projeyi Klonla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO = 'https://github.com/ahmetai-cell/pneumothorax-ai-detection'\n",
    "PROJECT_DIR = '/content/pneumothorax-ai-detection'\n",
    "\n",
    "if os.path.exists(PROJECT_DIR):\n",
    "    !cd {PROJECT_DIR} && git pull\n",
    "else:\n",
    "    !git clone {REPO}\n",
    "\n",
    "os.chdir(PROJECT_DIR)\n",
    "print('√áalƒ±≈üma dizini:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baƒüƒ±mlƒ±lƒ±klarƒ± Y√ºkle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \\\n",
    "    segmentation-models-pytorch \\\n",
    "    albumentations \\\n",
    "    pydicom \\\n",
    "    pynrrd \\\n",
    "    wandb \\\n",
    "    tqdm \\\n",
    "    fpdf2 \\\n",
    "    plotly\n",
    "\n",
    "print('‚úì Kurulum tamamlandƒ±')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Kaggle API Token Ayarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "KAGGLE_TOKEN = 'KGAT_ab6d3f32951d02e0cc877e406e207b72'\n",
    "KAGGLE_USER  = 'salihekmen'\n",
    "\n",
    "os.environ['KAGGLE_API_TOKEN'] = KAGGLE_TOKEN\n",
    "os.environ['KAGGLE_USERNAME']  = KAGGLE_USER\n",
    "\n",
    "kaggle_dir = os.path.expanduser('~/.kaggle')\n",
    "os.makedirs(kaggle_dir, exist_ok=True)\n",
    "with open(f'{kaggle_dir}/kaggle.json', 'w') as f:\n",
    "    json.dump({'username': KAGGLE_USER, 'key': KAGGLE_TOKEN}, f)\n",
    "os.chmod(f'{kaggle_dir}/kaggle.json', 0o600)\n",
    "\n",
    "print('‚úì Kaggle token ayarlandƒ±')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. SIIM-ACR Pneumothorax ‚Äî Drive'dan Y√ºkle veya ƒ∞ndir"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\n\nSIIM_LOCAL = 'data/raw/global/siim'\nos.makedirs(SIIM_LOCAL, exist_ok=True)\n\n# ‚îÄ‚îÄ 1) Drive'da veri var mƒ±? ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ndrive_count = int(subprocess.check_output(\n    f'find {DRIVE_SIIM} -name \"*.dcm\" 2>/dev/null | wc -l', shell=True\n).decode().strip())\n\nif drive_count > 1000:\n    print(f'‚úì Drive\\'da {drive_count:,} SIIM DICOM bulundu ‚Äî symlink olu≈üturuluyor...')\n    if not os.path.islink(SIIM_LOCAL):\n        os.rmdir(SIIM_LOCAL)\n        os.symlink(DRIVE_SIIM, SIIM_LOCAL)\n    print(f'‚úì {SIIM_LOCAL} ‚Üí {DRIVE_SIIM}')\n\n# ‚îÄ‚îÄ 2) Drive'da yok ‚Üí Kaggle dataset mirror'dan indir ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nelse:\n    print('Drive\\'da SIIM bulunamadƒ± ‚Äî Kaggle dataset\\'ten indiriliyor (~12 GB)...')\n    print('‚ö†Ô∏è  Bu i≈ülem ~20-40 dakika s√ºrebilir, bir kez yapƒ±lƒ±r.')\n    !kaggle datasets download \\\n        -d jesperdramsch/siim-acr-pneumothorax-segmentation-data \\\n        -p {DRIVE_SIIM} \\\n        --unzip\n    # Symlink\n    if not os.path.islink(SIIM_LOCAL):\n        os.rmdir(SIIM_LOCAL)\n        os.symlink(DRIVE_SIIM, SIIM_LOCAL)\n    downloaded = int(subprocess.check_output(\n        f'find {DRIVE_SIIM} -name \"*.dcm\" | wc -l', shell=True\n    ).decode().strip())\n    print(f'‚úì ƒ∞ndirme tamamlandƒ±: {downloaded:,} DICOM ‚Äî Drive\\'a kaydedildi')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Manifest Olu≈ütur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/data_manager.py --convert_rle\n!python scripts/data_manager.py --build_manifest"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. W&B Giri≈ü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q --upgrade wandb\n\nimport wandb\nWANDB_KEY = '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'\nwandb.login(key=WANDB_KEY, relogin=True)\nprint('‚úì W&B baƒülantƒ±sƒ± tamam')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pre-training Ba≈ülat\n",
    "\n",
    "**T4 GPU'da tahmini s√ºre:**\n",
    "- 1 epoch (112k g√∂r√ºnt√º, batch=32): ~14 dakika\n",
    "- 50 epoch: ~12 saat\n",
    "\n",
    "> ‚ö†Ô∏è Colab oturumu ~12 saatte kapanabilir. Checkpoint her fold'dan sonra kaydedilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/train_global.py \\\n    --sources SIIM \\\n    --encoder efficientnet-b0 \\\n    --img_size 512 \\\n    --epochs 30 \\\n    --batch_size 16 \\\n    --num_folds 5 \\\n    --lr 1e-4 \\\n    --no_wandb \\\n    --checkpoint_dir {DRIVE_CKPT}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Checkpoint'i Drive'a Kopyala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, glob\n",
    "\n",
    "base_model = 'checkpoints/global_base_model.pth'\n",
    "if os.path.exists(base_model):\n",
    "    shutil.copy(base_model, f'{DRIVE_CKPT}/global_base_model.pth')\n",
    "    shutil.copy('checkpoints/global_base_model_meta.json',\n",
    "                f'{DRIVE_CKPT}/global_base_model_meta.json')\n",
    "    print('‚úì Base model Drive\\'a kopyalandƒ±')\n",
    "\n",
    "for ckpt in glob.glob('checkpoints/global_folds/*.pth'):\n",
    "    shutil.copy(ckpt, DRIVE_CKPT)\n",
    "    print(f'  ‚Üí {os.path.basename(ckpt)}')\n",
    "\n",
    "results_csv = 'results/global_kfold_results.csv'\n",
    "if os.path.exists(results_csv):\n",
    "    shutil.copy(results_csv, DRIVE_CKPT)\n",
    "    print('‚úì Results CSV kopyalandƒ±')\n",
    "\n",
    "print(f'\\n‚úì T√ºm dosyalar Drive\\'a kaydedildi: {DRIVE_CKPT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sonu√ßlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('results/global_kfold_results.csv')\n",
    "    print('=== K-FOLD SONU√áLARI ===')\n",
    "    print(df.to_string(index=False))\n",
    "    print(f'\\nOrtalama Dice : {df[\"best_dice\"].mean():.4f} ¬± {df[\"best_dice\"].std():.4f}')\n",
    "    print(f'Ortalama AUC  : {df[\"best_auc\"].mean():.4f} ¬± {df[\"best_auc\"].std():.4f}')\n",
    "except FileNotFoundError:\n",
    "    print('Results CSV bulunamadƒ± ‚Äî eƒüitim hen√ºz bitmemi≈ü olabilir')\n",
    "\n",
    "try:\n",
    "    meta = json.load(open('checkpoints/global_base_model_meta.json'))\n",
    "    print('\\n=== BASE MODEL ===')\n",
    "    for k, v in meta.items():\n",
    "        print(f'  {k}: {v}')\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Sonraki Adƒ±m ‚Äî Fine-tuning\n",
    "\n",
    "DEU DICOM + NRRD verileri geldiƒüinde:\n",
    "\n",
    "```bash\n",
    "# Checkpoint'i Drive'dan al\n",
    "cp /content/drive/MyDrive/tubitak_pneumothorax/checkpoints/global_base_model.pth checkpoints/\n",
    "\n",
    "# DEU verilerini koy\n",
    "# data/local/dicom/*.dcm\n",
    "# data/local/nrrd/*.nrrd\n",
    "\n",
    "# Fine-tune et (~1-2 saat)\n",
    "python scripts/fine_tune_local.py --freeze_encoder --epochs 20\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}