{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Pneumothorax AI \u2014 Global Pre-training (T\u00dcB\u0130TAK 2209-A)"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83e\udec1 Pneumothorax AI \u2014 Global Pre-training\n",
    "**T\u00dcB\u0130TAK 2209-A | Ahmet Demir | Dokuz Eyl\u00fcl \u00dcniversitesi**\n",
    "\n",
    "Bu notebook:\n",
    "1. GitHub'dan projeyi klonlar\n",
    "2. Kaggle NIH ChestX-ray14 veri setini indirir\n",
    "3. U-Net++ modelini 50 epoch e\u011fitir\n",
    "4. En iyi checkpoint'i Google Drive'a kaydeder\n",
    "\n",
    "**Tahmini s\u00fcre:** T4 GPU ile ~12 saat (50 epoch, 112k g\u00f6r\u00fcnt\u00fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. GPU Kontrol\u00fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'YOK \u2014 Runtime > GPU se\u00e7!')\n",
    "print('CUDA:', torch.version.cuda)\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Projeyi Klonla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n\nREPO = 'https://github.com/ahmetai-cell/pneumothorax-ai-detection'\nPROJECT_DIR = '/content/pneumothorax-ai-detection'\n\nif os.path.exists(PROJECT_DIR):\n    !cd {PROJECT_DIR} && git pull\nelse:\n    !git clone {REPO}\n\nos.chdir(PROJECT_DIR)\nprint('\u00c7al\u0131\u015fma dizini:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ba\u011f\u0131ml\u0131l\u0131klar\u0131 Y\u00fckle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \\\n",
    "    segmentation-models-pytorch \\\n",
    "    albumentations \\\n",
    "    pydicom \\\n",
    "    pynrrd \\\n",
    "    wandb \\\n",
    "    tqdm \\\n",
    "    fpdf2 \\\n",
    "    plotly\n",
    "\n",
    "print('\u2713 Kurulum tamamland\u0131')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kaggle API Token Ayarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Kaggle token'\u0131n\u0131 buraya yap\u0131\u015ft\u0131r:\n",
    "KAGGLE_TOKEN = 'KGAT_ab6d3f32951d02e0cc877e406e207b72'  # salihekmen hesab\u0131\n",
    "KAGGLE_USER  = 'salihekmen'\n",
    "\n",
    "os.environ['KAGGLE_API_TOKEN'] = KAGGLE_TOKEN\n",
    "os.environ['KAGGLE_USERNAME']  = KAGGLE_USER\n",
    "\n",
    "# kaggle.json olu\u015ftur (baz\u0131 ara\u00e7lar i\u00e7in gerekli)\n",
    "import json\n",
    "kaggle_dir = os.path.expanduser('~/.kaggle')\n",
    "os.makedirs(kaggle_dir, exist_ok=True)\n",
    "with open(f'{kaggle_dir}/kaggle.json', 'w') as f:\n",
    "    json.dump({'username': KAGGLE_USER, 'key': KAGGLE_TOKEN}, f)\n",
    "os.chmod(f'{kaggle_dir}/kaggle.json', 0o600)\n",
    "\n",
    "print('\u2713 Kaggle token ayarland\u0131')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NIH ChestX-ray14 \u0130ndir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIH_DIR = 'data/raw/global/nih'\n",
    "os.makedirs(NIH_DIR, exist_ok=True)\n",
    "\n",
    "# Zaten indirilmi\u015fse atla\n",
    "nih_images = !find {NIH_DIR} -name '*.png' 2>/dev/null | wc -l\n",
    "n_images = int(nih_images[0].strip())\n",
    "\n",
    "if n_images > 100000:\n",
    "    print(f'\u2713 NIH zaten mevcut: {n_images:,} g\u00f6r\u00fcnt\u00fc')\n",
    "else:\n",
    "    print(f'NIH indiriliyor (~42 GB)...')\n",
    "    !kaggle datasets download \\\n",
    "        -d nih-chest-xrays/data \\\n",
    "        -p {NIH_DIR} \\\n",
    "        --unzip\n",
    "    nih_images = !find {NIH_DIR} -name '*.png' | wc -l\n",
    "    print(f'\u2713 \u0130ndirme tamamland\u0131: {nih_images[0].strip()} g\u00f6r\u00fcnt\u00fc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Manifest Olu\u015ftur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/data_manager.py --build_manifest\n",
    "!python scripts/unify_annotations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. W&B Giri\u015f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# wandb.ai/ahmet-ai-t-bi-tak hesab\u0131nla giri\u015f yap\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Google Drive Ba\u011fla (Checkpoint Kaydet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_CKPT = '/content/drive/MyDrive/tubitak_pneumothorax/checkpoints'\n",
    "os.makedirs(DRIVE_CKPT, exist_ok=True)\n",
    "print(f'\u2713 Drive ba\u011fland\u0131: {DRIVE_CKPT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pre-training Ba\u015flat\n",
    "\n",
    "**T4 GPU'da tahmini s\u00fcre:**\n",
    "- 1 epoch (112k g\u00f6r\u00fcnt\u00fc, batch=32): ~14 dakika\n",
    "- 50 epoch: ~12 saat\n",
    "\n",
    "> \u26a0\ufe0f Colab oturumu ~12 saatte kapanabilir. Checkpoint her fold'dan sonra kaydedilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/train_global.py \\\n",
    "    --sources NIH \\\n",
    "    --encoder efficientnet-b0 \\\n",
    "    --img_size 512 \\\n",
    "    --epochs 50 \\\n",
    "    --batch_size 32 \\\n",
    "    --num_folds 5 \\\n",
    "    --lr 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Checkpoint'i Drive'a Kopyala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, glob\n",
    "\n",
    "# Base model\n",
    "base_model = 'checkpoints/global_base_model.pth'\n",
    "if os.path.exists(base_model):\n",
    "    shutil.copy(base_model, f'{DRIVE_CKPT}/global_base_model.pth')\n",
    "    shutil.copy('checkpoints/global_base_model_meta.json',\n",
    "                f'{DRIVE_CKPT}/global_base_model_meta.json')\n",
    "    print(f'\u2713 Base model Drive\\'a kopyaland\u0131')\n",
    "\n",
    "# T\u00fcm fold checkpoint'leri\n",
    "for ckpt in glob.glob('checkpoints/global_folds/*.pth'):\n",
    "    shutil.copy(ckpt, DRIVE_CKPT)\n",
    "    print(f'  \u2192 {os.path.basename(ckpt)}')\n",
    "\n",
    "# Results CSV\n",
    "results_csv = 'results/global_kfold_results.csv'\n",
    "if os.path.exists(results_csv):\n",
    "    shutil.copy(results_csv, DRIVE_CKPT)\n",
    "    print(f'\u2713 Results CSV kopyaland\u0131')\n",
    "\n",
    "print('\\n\u2713 T\u00fcm dosyalar Drive\\'a kaydedildi:', DRIVE_CKPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sonu\u00e7lar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# K-Fold sonu\u00e7lar\u0131\n",
    "try:\n",
    "    df = pd.read_csv('results/global_kfold_results.csv')\n",
    "    print('=== K-FOLD SONU\u00c7LARI ===')\n",
    "    print(df.to_string(index=False))\n",
    "    print(f'\\nOrtalama Dice : {df[\"best_dice\"].mean():.4f} \u00b1 {df[\"best_dice\"].std():.4f}')\n",
    "    print(f'Ortalama AUC  : {df[\"best_auc\"].mean():.4f} \u00b1 {df[\"best_auc\"].std():.4f}')\n",
    "except FileNotFoundError:\n",
    "    print('Results CSV bulunamad\u0131 \u2014 e\u011fitim hen\u00fcz bitmemi\u015f olabilir')\n",
    "\n",
    "# Base model meta\n",
    "try:\n",
    "    meta = json.load(open('checkpoints/global_base_model_meta.json'))\n",
    "    print('\\n=== BASE MODEL ===')\n",
    "    for k, v in meta.items():\n",
    "        print(f'  {k}: {v}')\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \u2705 Sonraki Ad\u0131m \u2014 Fine-tuning\n",
    "\n",
    "DEU DICOM + NRRD verileri geldi\u011finde:\n",
    "\n",
    "```bash\n",
    "# Checkpoint'i Drive'dan al\n",
    "cp /content/drive/MyDrive/tubitak_pneumothorax/checkpoints/global_base_model.pth checkpoints/\n",
    "\n",
    "# DEU verilerini koy\n",
    "# data/local/dicom/*.dcm\n",
    "# data/local/nrrd/*.nrrd\n",
    "\n",
    "# Fine-tune et (~1-2 saat)\n",
    "python scripts/fine_tune_local.py --freeze_encoder --epochs 20\n",
    "```"
   ]
  }
 ]
}