{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Pneumothorax AI ‚Äî Global Pre-training (T√úBƒ∞TAK 2209-A)"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü´Å Pneumothorax AI ‚Äî Global Pre-training\n",
    "**T√úBƒ∞TAK 2209-A | Ahmet Demir | Dokuz Eyl√ºl √úniversitesi**\n",
    "\n",
    "Bu notebook:\n",
    "1. GitHub'dan projeyi klonlar\n",
    "2. Kaggle NIH ChestX-ray14 veri setini indirir\n",
    "3. U-Net++ modelini 50 epoch eƒüitir\n",
    "4. En iyi checkpoint'i Google Drive'a kaydeder\n",
    "\n",
    "**Tahmini s√ºre:** T4 GPU ile ~12 saat (50 epoch, 112k g√∂r√ºnt√º)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. GPU Kontrol√º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'YOK ‚Äî Runtime > GPU se√ß!')\n",
    "print('CUDA:', torch.version.cuda)\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Projeyi Klonla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO = 'https://github.com/Ahmet-demir-ai/pneumothorax-ai-detection'\n",
    "PROJECT_DIR = '/content/pneumothorax-ai-detection'\n",
    "\n",
    "if os.path.exists(PROJECT_DIR):\n",
    "    !cd {PROJECT_DIR} && git pull\n",
    "else:\n",
    "    !git clone {REPO}\n",
    "\n",
    "os.chdir(PROJECT_DIR)\n",
    "print('√áalƒ±≈üma dizini:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baƒüƒ±mlƒ±lƒ±klarƒ± Y√ºkle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \\\n",
    "    segmentation-models-pytorch \\\n",
    "    albumentations \\\n",
    "    pydicom \\\n",
    "    pynrrd \\\n",
    "    wandb \\\n",
    "    tqdm \\\n",
    "    fpdf2 \\\n",
    "    plotly\n",
    "\n",
    "print('‚úì Kurulum tamamlandƒ±')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kaggle API Token Ayarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Kaggle token'ƒ±nƒ± buraya yapƒ±≈ütƒ±r:\n",
    "KAGGLE_TOKEN = 'KGAT_ab6d3f32951d02e0cc877e406e207b72'  # salihekmen hesabƒ±\n",
    "KAGGLE_USER  = 'salihekmen'\n",
    "\n",
    "os.environ['KAGGLE_API_TOKEN'] = KAGGLE_TOKEN\n",
    "os.environ['KAGGLE_USERNAME']  = KAGGLE_USER\n",
    "\n",
    "# kaggle.json olu≈ütur (bazƒ± ara√ßlar i√ßin gerekli)\n",
    "import json\n",
    "kaggle_dir = os.path.expanduser('~/.kaggle')\n",
    "os.makedirs(kaggle_dir, exist_ok=True)\n",
    "with open(f'{kaggle_dir}/kaggle.json', 'w') as f:\n",
    "    json.dump({'username': KAGGLE_USER, 'key': KAGGLE_TOKEN}, f)\n",
    "os.chmod(f'{kaggle_dir}/kaggle.json', 0o600)\n",
    "\n",
    "print('‚úì Kaggle token ayarlandƒ±')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NIH ChestX-ray14 ƒ∞ndir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIH_DIR = 'data/raw/global/nih'\n",
    "os.makedirs(NIH_DIR, exist_ok=True)\n",
    "\n",
    "# Zaten indirilmi≈üse atla\n",
    "nih_images = !find {NIH_DIR} -name '*.png' 2>/dev/null | wc -l\n",
    "n_images = int(nih_images[0].strip())\n",
    "\n",
    "if n_images > 100000:\n",
    "    print(f'‚úì NIH zaten mevcut: {n_images:,} g√∂r√ºnt√º')\n",
    "else:\n",
    "    print(f'NIH indiriliyor (~42 GB)...')\n",
    "    !kaggle datasets download \\\n",
    "        -d nih-chest-xrays/data \\\n",
    "        -p {NIH_DIR} \\\n",
    "        --unzip\n",
    "    nih_images = !find {NIH_DIR} -name '*.png' | wc -l\n",
    "    print(f'‚úì ƒ∞ndirme tamamlandƒ±: {nih_images[0].strip()} g√∂r√ºnt√º')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Manifest Olu≈ütur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/data_manager.py --build_manifest\n",
    "!python scripts/unify_annotations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. W&B Giri≈ü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# wandb.ai/ahmet-ai-t-bi-tak hesabƒ±nla giri≈ü yap\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Google Drive Baƒüla (Checkpoint Kaydet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_CKPT = '/content/drive/MyDrive/tubitak_pneumothorax/checkpoints'\n",
    "os.makedirs(DRIVE_CKPT, exist_ok=True)\n",
    "print(f'‚úì Drive baƒülandƒ±: {DRIVE_CKPT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pre-training Ba≈ülat\n",
    "\n",
    "**T4 GPU'da tahmini s√ºre:**\n",
    "- 1 epoch (112k g√∂r√ºnt√º, batch=32): ~14 dakika\n",
    "- 50 epoch: ~12 saat\n",
    "\n",
    "> ‚ö†Ô∏è Colab oturumu ~12 saatte kapanabilir. Checkpoint her fold'dan sonra kaydedilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/train_global.py \\\n",
    "    --sources NIH \\\n",
    "    --encoder efficientnet-b0 \\\n",
    "    --img_size 512 \\\n",
    "    --epochs 50 \\\n",
    "    --batch_size 32 \\\n",
    "    --num_folds 5 \\\n",
    "    --lr 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Checkpoint'i Drive'a Kopyala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, glob\n",
    "\n",
    "# Base model\n",
    "base_model = 'checkpoints/global_base_model.pth'\n",
    "if os.path.exists(base_model):\n",
    "    shutil.copy(base_model, f'{DRIVE_CKPT}/global_base_model.pth')\n",
    "    shutil.copy('checkpoints/global_base_model_meta.json',\n",
    "                f'{DRIVE_CKPT}/global_base_model_meta.json')\n",
    "    print(f'‚úì Base model Drive\\'a kopyalandƒ±')\n",
    "\n",
    "# T√ºm fold checkpoint'leri\n",
    "for ckpt in glob.glob('checkpoints/global_folds/*.pth'):\n",
    "    shutil.copy(ckpt, DRIVE_CKPT)\n",
    "    print(f'  ‚Üí {os.path.basename(ckpt)}')\n",
    "\n",
    "# Results CSV\n",
    "results_csv = 'results/global_kfold_results.csv'\n",
    "if os.path.exists(results_csv):\n",
    "    shutil.copy(results_csv, DRIVE_CKPT)\n",
    "    print(f'‚úì Results CSV kopyalandƒ±')\n",
    "\n",
    "print('\\n‚úì T√ºm dosyalar Drive\\'a kaydedildi:', DRIVE_CKPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sonu√ßlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# K-Fold sonu√ßlarƒ±\n",
    "try:\n",
    "    df = pd.read_csv('results/global_kfold_results.csv')\n",
    "    print('=== K-FOLD SONU√áLARI ===')\n",
    "    print(df.to_string(index=False))\n",
    "    print(f'\\nOrtalama Dice : {df[\"best_dice\"].mean():.4f} ¬± {df[\"best_dice\"].std():.4f}')\n",
    "    print(f'Ortalama AUC  : {df[\"best_auc\"].mean():.4f} ¬± {df[\"best_auc\"].std():.4f}')\n",
    "except FileNotFoundError:\n",
    "    print('Results CSV bulunamadƒ± ‚Äî eƒüitim hen√ºz bitmemi≈ü olabilir')\n",
    "\n",
    "# Base model meta\n",
    "try:\n",
    "    meta = json.load(open('checkpoints/global_base_model_meta.json'))\n",
    "    print('\\n=== BASE MODEL ===')\n",
    "    for k, v in meta.items():\n",
    "        print(f'  {k}: {v}')\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Sonraki Adƒ±m ‚Äî Fine-tuning\n",
    "\n",
    "DEU DICOM + NRRD verileri geldiƒüinde:\n",
    "\n",
    "```bash\n",
    "# Checkpoint'i Drive'dan al\n",
    "cp /content/drive/MyDrive/tubitak_pneumothorax/checkpoints/global_base_model.pth checkpoints/\n",
    "\n",
    "# DEU verilerini koy\n",
    "# data/local/dicom/*.dcm\n",
    "# data/local/nrrd/*.nrrd\n",
    "\n",
    "# Fine-tune et (~1-2 saat)\n",
    "python scripts/fine_tune_local.py --freeze_encoder --epochs 20\n",
    "```"
   ]
  }
 ]
}
